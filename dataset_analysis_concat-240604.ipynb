{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from IPython.display import FileLink\n",
    "import pandas as pd\n",
    "import re\n",
    "from IPython.display import display\n",
    "from ipyfilechooser import FileChooser\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "from tabulate import tabulate\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "from pathlib import Path\n",
    "\n",
    "import json\n",
    "\n",
    "# ignore group warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", 'This pattern has match groups')\n",
    "warnings.filterwarnings(\"ignore\", 'This pattern is interpreted as a regular expression, and has match groups')\n",
    "\n",
    "# Add complete filepath here.\n",
    "filepath = sys.argv[1] if len(sys.argv) > 1 else ''\n",
    "# filepath = \"/Users/kazkas/Downloads/IPCCEDGAR_EF_MVP02_20240604.csv\"\n",
    "zip = sys.argv[2].lower() == 'true' if len(sys.argv) > 2 else False\n",
    "\n",
    "filename = Path(filepath).stem\n",
    "folder_to_download = filename  # Use the same name as the CSV file you are feeding in. At the very bottom of the page, once the report is generated, a Zip file becomes available to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the JSON file and load columns into variables\n",
    "with open(\"columns.json\", 'r') as json_file:\n",
    "    # Load the JSON data into a Python object\n",
    "    data = json.load(json_file)\n",
    "    universal_tabular_columns = data[\"universal_tabular_columns\"]\n",
    "    non_discerning_columns = data[\"non_discerning_columns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 557362: unexpected end of data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder path : /Users/kazkas/Desktop/T_CIRCA-Desktop/2024/CODE/DATACHECKER/IPCCEDGAR_EF_MVP02_20240604\n"
     ]
    }
   ],
   "source": [
    "# change if there is an error reading csv or it is strangely formatted\n",
    "# df = pd.read_csv(filepath, on_bad_lines='skip')\n",
    "# df = pd.read_csv(filepath)\n",
    "df = pd.read_csv(filepath, engine=\"python\", encoding='utf-8', on_bad_lines='warn')\n",
    "# df = pd.read_csv(filepath, sep='delimiter', header=None)\n",
    "\n",
    "# add index to match the csv\n",
    "df.index += 2\n",
    "\n",
    "# print(df.columns)\n",
    "\n",
    "# set current date\n",
    "today = date.today()\n",
    "\n",
    "# YYmmdd\n",
    "CURRENT_DATE = today.strftime(\"%Y%m%d\")\n",
    "\n",
    "# CREATE FOLDERS\n",
    "# create a folder for each document\n",
    "FOLDER_NAME = re.sub(\"[^A-Z0-9]\", \" \", filename,0,re.IGNORECASE)\n",
    "FOLDER_NAME = re.sub(' +', '_', FOLDER_NAME)\n",
    "\n",
    "FOLDER_PATH = os.path.join(os.getcwd(), FOLDER_NAME)\n",
    "print(f'Folder path : {FOLDER_PATH}')\n",
    "if not os.path.exists(FOLDER_PATH):\n",
    "    os.mkdir(FOLDER_PATH)\n",
    "\n",
    "# formatting\n",
    "if not os.path.exists(f\"{FOLDER_PATH}/FORMATTING\"):\n",
    "    os.mkdir(f\"{FOLDER_PATH}/FORMATTING\")\n",
    "\n",
    "# repetition\n",
    "if not os.path.exists(f\"{FOLDER_PATH}/REPETITION\"):\n",
    "    os.mkdir(f\"{FOLDER_PATH}/REPETITION\")\n",
    "\n",
    "# other\n",
    "if not os.path.exists(f\"{FOLDER_PATH}/OTHER\"):\n",
    "    os.mkdir(f\"{FOLDER_PATH}/OTHER\")\n",
    "\n",
    "# PRINT TABLE\n",
    "MASTER_TABLE = \"\"\n",
    "CMD_TABLE= []\n",
    "def add_to_cmd_table(name, value, mandatory=False):\n",
    "    global CMD_TABLE\n",
    "    if mandatory:\n",
    "        value = \"\\033[1;31;40m\" + value + \"\\033[0m\"\n",
    "    CMD_TABLE.append([name, value])\n",
    "\n",
    "def print_table(table, horizontal, column_headers = []):\n",
    "    global MASTER_TABLE\n",
    "    tab = None\n",
    "    if horizontal:\n",
    "        tab = PrettyTable(table[0])\n",
    "        tab.add_rows(table[1:])\n",
    "    else:\n",
    "        tab = PrettyTable()\n",
    "        for index, item in enumerate(column_headers):\n",
    "            tab.add_column(item, table[index])\n",
    "    tab.padding_width = 2\n",
    "    tab.junction_char = '.'\n",
    "    # print(tab)\n",
    "    MASTER_TABLE += tab.get_csv_string()\n",
    "    MASTER_TABLE += \"-\\n\"\n",
    "    MASTER_TABLE += \"-\\n\"\n",
    "    MASTER_TABLE += \"-\\n\"\n",
    "\n",
    "# print(df.iloc[df.shape[0]-1])\n",
    "# print(df.shape[0])\n",
    "# # Drop rows with any empty cells // put more threshold to kick the strange ones\n",
    "# df.dropna(how='all',thresh=1, inplace=True)\n",
    "# print(df.shape[0])\n",
    "\n",
    "MASTER_TABLE += \"Total data points\\n\"\n",
    "MASTER_TABLE += f\"{df.shape[0]}\\n\"\n",
    "MASTER_TABLE += \"-\\n\"\n",
    "MASTER_TABLE += \"-\\n\"\n",
    "MASTER_TABLE += \"-\\n\"\n",
    "\n",
    "# print(len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether existing columns are in the same names as universal tabular columns\n",
    "for index, item in enumerate(df.columns):\n",
    "    if item not in universal_tabular_columns:\n",
    "        print(f'this is not the right name : {item}')\n",
    "\n",
    "# add missing columns \n",
    "for index, item in enumerate(universal_tabular_columns):\n",
    "    if item not in df.columns:\n",
    "        print(f'the column was added : {item}')\n",
    "        # df[item] = pd.Series()\n",
    "        df[item] = pd.Series(dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicates_non_discerning(dataFrame, df_columns, table_text, analyse_name):\n",
    "    global MASTER_TABLE\n",
    "    # finds all duplicate rows and keeps all of them\n",
    "    df_local = dataFrame[df_columns]\n",
    "    df_range = df_local[df_local.duplicated(keep=False)]\n",
    "\n",
    "    # sort based on the first column\n",
    "    df_range = df_range.sort_values(by=[df_columns[0]])\n",
    "\n",
    "    # total dataframe count\n",
    "    df_count = dataFrame.shape[0]\n",
    "    \n",
    "    # Number of instances\n",
    "    df_instance_count = df_range.shape[0]\n",
    "\n",
    "    # Number of records that would be removed\n",
    "    df_without_range = df_local.drop_duplicates()\n",
    "    df_range_count = df_count - df_without_range.shape[0]\n",
    "\n",
    "    # - % of total data affected (copies/total)\n",
    "    df_range_percent = df_range_count / df_count\n",
    "\n",
    "    # print(f\"{analyse_name}\")\n",
    "    MASTER_TABLE += f\"{analyse_name}\\n\"\n",
    "\n",
    "    # if there are no values exit\n",
    "    if df_range_percent == 0:\n",
    "        MASTER_TABLE += \"PASS\\n\"\n",
    "        MASTER_TABLE += \"-\\n\"\n",
    "        MASTER_TABLE += \"-\\n\"\n",
    "        MASTER_TABLE += \"-\\n\"\n",
    "        add_to_cmd_table(analyse_name, \"PASS\")\n",
    "        return\n",
    "    add_to_cmd_table(analyse_name, \"FIX\", True)\n",
    "    \n",
    "    table = [table_text, [df_instance_count, df_range_count, df_range_percent]]\n",
    "    print_table(table, True)\n",
    "\n",
    "    # SAVE FILES\n",
    "    # save - Rows that are duplicated - OUTPUT 1\n",
    "    df_range_full_table = dataFrame.loc[df_range.index]\n",
    "    df_range_full_table.to_csv(f'{FOLDER_PATH}/OTHER/Report-{analyse_name}-Output_1-{filename}-{CURRENT_DATE}.csv',encoding='utf-8')\n",
    "    \n",
    "    # print(f\"Saved â€” {analyse_name}-Report-Output_1\")\n",
    "\n",
    "    # save - Duplicate instances & number of each - OUTPUT 2\n",
    "    # count the duplicates\n",
    "    df_range_group = df_range.groupby(df_range.columns.tolist(),as_index=False, dropna=False, sort=True).size()\n",
    "    # add count to begining\n",
    "    df_range_group.insert(0, \"count\", df_range_group[\"size\"])\n",
    "    # remove size at the end\n",
    "    df_range_group.pop('size')\n",
    "    # sort values by count\n",
    "    df_range_group = df_range_group.sort_values(by=\"count\", ascending=False)\n",
    "    # reset index\n",
    "    df_range_group = df_range_group.reset_index(drop=True)\n",
    "    # save\n",
    "    df_range_group.to_csv(f'{FOLDER_PATH}/OTHER/Report-{analyse_name}-Output_2-{filename}-{CURRENT_DATE}.csv',encoding='utf-8')\n",
    "\n",
    "# DUPLICATES\n",
    "table_text = ['Number of instances', 'Number of records that would be removed', '% of total data affected (copies/total)']\n",
    "duplicates_non_discerning(df, df.columns, table_text, \"Duplication\")\n",
    "\n",
    "# drop duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# NON DISCERNING-INFO\n",
    "table_text = ['Number of instances', 'Number of records that indiscernable', '% of rows of the total data affected (copies/total)']\n",
    "duplicates_non_discerning(df, non_discerning_columns, table_text, \"Non_discerning_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_value(dataFrame):\n",
    "    global MASTER_TABLE\n",
    "\n",
    "    # total dataframe count\n",
    "    df_count = dataFrame.shape[0]\n",
    "\n",
    "    def change_missing_values(mvv):\n",
    "        mvv_changed = []\n",
    "        for i in mvv:\n",
    "            val = i\n",
    "            if i == 0:\n",
    "                val = \"full\"\n",
    "            elif i == df_count:\n",
    "                val = \"---\"\n",
    "            mvv_changed.append(val)\n",
    "                \n",
    "        return mvv_changed\n",
    "\n",
    "    df_mandatory_ef_columns = [\"Name\", \"Value\", \"Unit\"] # Emission Factors\n",
    "    df_mandatory_c_columns = [\"Name\", \"Components\"] # Compositions\n",
    "\n",
    "    # check whether it is emmission factor or composition\n",
    "    if dataFrame[\"Value\"].isna().sum() == df_count:\n",
    "        df_mandatory_columns= df_mandatory_c_columns\n",
    "    else:\n",
    "        df_mandatory_columns= df_mandatory_ef_columns\n",
    "    df_optional = dataFrame.drop(columns = df_mandatory_columns) # mandatory dataframe\n",
    "    df_mandatory = dataFrame[df_mandatory_columns] # mandatory dataframe\n",
    "    \n",
    "    # MANDATORY\n",
    "    missing_value_table = df_mandatory.isna().sum()\n",
    "    missing_value_values = change_missing_values(missing_value_table.tolist())\n",
    "    \n",
    "    # save misisng value table\n",
    "    MASTER_TABLE += f\"Missing_value_mandatory\\n\"\n",
    "\n",
    "    # if it does not miss any values\n",
    "    if len([x for x in missing_value_values if x != \"full\"]) == 0:\n",
    "        MASTER_TABLE += \"PASS\\n\"\n",
    "        MASTER_TABLE += \"-\\n\"\n",
    "        MASTER_TABLE += \"-\\n\"\n",
    "        MASTER_TABLE += \"-\\n\"\n",
    "        add_to_cmd_table(\"Missing_value_mandatory\", \"PASS\")\n",
    "    else:\n",
    "        table = [missing_value_table.index.tolist(), missing_value_values]\n",
    "        print_table(table, False, [\"Column\", \"Count\"])\n",
    "        add_to_cmd_table(\"Missing_value_mandatory\", \"FIX\", True)\n",
    "\n",
    "        # go trough each column in the dataFrame and check which columns misses somve values. If it does, save the document\n",
    "        for col_name, val in missing_value_table.items():\n",
    "            if val != dataFrame.shape[0] and val != 0:\n",
    "                cn = re.sub(\"[^A-Z]\", \" \", col_name,0,re.IGNORECASE)\n",
    "                cn = re.sub(' +', '_', cn)\n",
    "                fname = f'{FOLDER_PATH}/OTHER/Report-Missing_value_mandatory-{cn}-Output_1-{filename}-{CURRENT_DATE}.csv'\n",
    "                df_isna = dataFrame[dataFrame[col_name].isna()]\n",
    "                df_isna.to_csv(fname,encoding='utf-8')\n",
    "    \n",
    "    # OPTIONAL\n",
    "    missing_value_table = df_optional.isna().sum()\n",
    "    missing_value_values = change_missing_values(missing_value_table.tolist())\n",
    "    \n",
    "    # save misisng value table\n",
    "    MASTER_TABLE += f\"Missing_value_optional\\n\"\n",
    "\n",
    "    # if it does not miss any values\n",
    "    table = [missing_value_table.index.tolist(), missing_value_values]\n",
    "    print_table(table, False, [\"Column\", \"Count\"])\n",
    "\n",
    "    # go trough each column in the dataFrame and check which columns misses somve values. If it does, save the document\n",
    "    for col_name, val in missing_value_table.items():\n",
    "        if val != dataFrame.shape[0] and val != 0:\n",
    "            cn = re.sub(\"[^A-Z]\", \" \", col_name,0,re.IGNORECASE)\n",
    "            cn = re.sub(' +', '_', cn)\n",
    "            fname = f'{FOLDER_PATH}/OTHER/Report-Missing_value_optional-{cn}-Output_1-{filename}-{CURRENT_DATE}.csv'\n",
    "            df_isna = dataFrame[dataFrame[col_name].isna()]\n",
    "            df_isna.to_csv(fname,encoding='utf-8')\n",
    "\n",
    "\n",
    "missing_value(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repetition(dataFrame, df_columns, table_text, analyse_name):\n",
    "    global MASTER_TABLE\n",
    "    # finds all duplicate rows and keeps all of them\n",
    "    df_local = dataFrame[df_columns]\n",
    "    df_range = df_local[df_local.duplicated(keep=False)]\n",
    "\n",
    "    # sort based on the first column\n",
    "    df_range = df_range.sort_values(by=[df_columns[0]])\n",
    "\n",
    "    # total dataframe count\n",
    "    df_count = dataFrame.shape[0]\n",
    "\n",
    "    # count the duplicates\n",
    "    df_range_group = df_range.groupby(df_range.columns.tolist(),as_index=False, dropna=False, sort=True).size()\n",
    "    # sort values by count\n",
    "    df_range_group = df_range_group.sort_values(by=\"size\", ascending=False)\n",
    "    # reset index\n",
    "    df_range_group = df_range_group.reset_index(drop=True)\n",
    "\n",
    "    # print \n",
    "    MASTER_TABLE += f\"{analyse_name}\\n\"\n",
    "    MASTER_TABLE += f\"{table_text}\\n\"\n",
    "\n",
    "    # if there are no values exit\n",
    "    if df_range_group.shape[0] == 0:\n",
    "        MASTER_TABLE += \"PASS\\n\"\n",
    "        MASTER_TABLE += \"-\\n\"\n",
    "        MASTER_TABLE += \"-\\n\"\n",
    "        MASTER_TABLE += \"-\\n\"\n",
    "        add_to_cmd_table(analyse_name, \"PASS\")\n",
    "        return\n",
    "    \n",
    "    add_to_cmd_table(analyse_name, \"FIX\")\n",
    "\n",
    "    # go trough the range and add it to the list\n",
    "    df_range_vals = [2,5,10,25,50,100,1000,10000]\n",
    "    df_total = []\n",
    "    df_instances = []\n",
    "    df_percent = []\n",
    "    for index, value in enumerate(df_range_vals):\n",
    "        partition_of_data = df_range_group[df_range_group[\"size\"] >= value]\n",
    "        partition_of_data_total = partition_of_data.sum()[\"size\"]\n",
    "        partition_of_data_instances = len(partition_of_data[\"size\"])\n",
    "        partition_of_data_percent = partition_of_data_total/df_count\n",
    "\n",
    "        df_total.append(partition_of_data_total)\n",
    "        df_instances.append(partition_of_data_instances)\n",
    "        df_percent.append(partition_of_data_percent)\n",
    "\n",
    "    table = [df_range_vals,df_total,df_instances,df_percent]\n",
    "    print_table(table, False, [\"more or equal to copies \", \"data affected\", \"instance count\", \"percent\"])\n",
    "\n",
    "\n",
    "    # SAVE FILES\n",
    "    # save - Rows that are duplicated - OUTPUT 1\n",
    "    df_range_full_table = dataFrame.loc[df_range.index]\n",
    "    df_range_full_table.to_csv(f'{FOLDER_PATH}/REPETITION/Report-{analyse_name}-Output_1-{filename}-{CURRENT_DATE}.csv',encoding='utf-8')\n",
    "\n",
    "    # save - Duplicate instances & number of each - OUTPUT 2\n",
    "    # count the duplicates\n",
    "    df_range_group = df_range.groupby(df_range.columns.tolist(),as_index=False, dropna=False, sort=True).size()\n",
    "    # add count to begining\n",
    "    df_range_group.insert(0, \"count\", df_range_group[\"size\"])\n",
    "    # remove size at the end\n",
    "    df_range_group.pop('size')\n",
    "    # sort values by count\n",
    "    df_range_group = df_range_group.sort_values(by=\"count\", ascending=False)\n",
    "    # reset index\n",
    "    df_range_group = df_range_group.reset_index(drop=True)\n",
    "    # save\n",
    "    df_range_group.to_csv(f'{FOLDER_PATH}/REPETITION/Report-{analyse_name}-Output_2-{filename}-{CURRENT_DATE}.csv',encoding='utf-8')\n",
    "\n",
    "# REPETITION 1\n",
    "table_text = 'The Name instance'\n",
    "repetition(df, [\"Name\"], table_text, \"Repetition_1\")\n",
    "\n",
    "# REPETITION 2\n",
    "table_text = 'The Description instance'\n",
    "repetition(df, [\"Description\"], table_text, \"Repetition_2\")\n",
    "\n",
    "try:\n",
    "    # if (df[[\"Components\"]].isna().sum() == df.shape[0]).bool() == False:\n",
    "    if (df[[\"Components\"]].isnull().sum() == df.shape[0]).all() == False:\n",
    "        # REPETITION 3\n",
    "        table_text = 'The Comp. Items List instance'\n",
    "        repetition(df, [\"Components\"], table_text, \"Repetition_3\")\n",
    "    else:\n",
    "        # REPETITION 4\n",
    "        table_text = 'The Value instance'\n",
    "        repetition(df, [\"Value\"], table_text, \"Repetition_4\")\n",
    "except:\n",
    "    print(\"The dataset does not include all columns\")\n",
    "\n",
    "    # REPETITION 4\n",
    "    table_text = 'The Value instance'\n",
    "    repetition(df, [\"Value\"], table_text, \"Repetition_4\")\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Region/Regional Scope\n"
     ]
    }
   ],
   "source": [
    "def format_1(dataFrame, df_columns, analyse_name):\n",
    "    global MASTER_TABLE\n",
    "    \n",
    "    def check_spelling(item):\n",
    "        try:\n",
    "            row = item[df_columns[0]]\n",
    "            row_split = row.split()\n",
    "            for i in row_split:\n",
    "                if not i[0].isupper():\n",
    "                    if not i == \"of\" and not i == \"and\" and not i ==\"the\":\n",
    "                        return True\n",
    "            return False\n",
    "        except:\n",
    "            print(f\"Exception, inspect it : {item}\")\n",
    "            return True\n",
    "\n",
    "    df_local = dataFrame[df_columns]\n",
    "\n",
    "    df_results = df_local[df_local.apply(check_spelling, axis=1)]\n",
    "\n",
    "    # check upper case\n",
    "    count = df_results.shape[0]\n",
    "    indexes = df_results.index\n",
    "\n",
    "    # count percent\n",
    "    df_percent = count / df_local.shape[0]\n",
    "\n",
    "    # print table\n",
    "    MASTER_TABLE += f\"{analyse_name}\\n\"\n",
    "    \n",
    "    # if there are no values exit\n",
    "    if df_percent == 0:\n",
    "        MASTER_TABLE += \"% of rows where the Region/Regional Scope column items is without capitalization\\n\"\n",
    "        MASTER_TABLE += \"PASS\\n\"\n",
    "        MASTER_TABLE += \"-\\n\"\n",
    "        MASTER_TABLE += \"-\\n\"\n",
    "        MASTER_TABLE += \"-\\n\"\n",
    "\n",
    "        add_to_cmd_table(analyse_name, \"PASS\")\n",
    "        return\n",
    "    \n",
    "    add_to_cmd_table(analyse_name, \"FIX\", True)\n",
    "    \n",
    "    table = [[\"% of rows where the Region/Regional Scope column items is without capitalization\"],[df_percent]]\n",
    "    print_table(table, True)\n",
    "\n",
    "    # save document\n",
    "    df_full_table = dataFrame.loc[indexes]\n",
    "    df_full_table.to_csv(f'{FOLDER_PATH}/FORMATTING/Report-{analyse_name}-Output_1-{filename}-{CURRENT_DATE}.csv',encoding='utf-8')\n",
    "\n",
    "\n",
    "if (df[[\"Region/Regional Scope\"]].isnull().sum() == df.shape[0]).all() == False:            \n",
    "    format_1(df, [\"Region/Regional Scope\"], \"Formatting-1\")\n",
    "else:\n",
    "    print(\"No Region/Regional Scope\")\n",
    "    # print table\n",
    "    MASTER_TABLE += \"Formatting_1\\n\"\n",
    "    MASTER_TABLE += \"% of rows where the Region/Regional Scope column items is without capitalization\\n\"\n",
    "    MASTER_TABLE += \"No Region/Regional Scope \\n\"\n",
    "    MASTER_TABLE += \"-\\n\"\n",
    "    MASTER_TABLE += \"-\\n\"\n",
    "    MASTER_TABLE += \"-\\n\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_2(dataFrame, df_columns, analyse_name):\n",
    "    global MASTER_TABLE\n",
    "    \n",
    "    def check_numbers(item):\n",
    "        row = item[df_columns[0]]\n",
    "        if not isinstance(row,float) and not isinstance(row,int) and not isinstance(row, np.int64):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    df_local = dataFrame[df_columns]\n",
    "\n",
    "    df_results = df_local[df_local.apply(check_numbers, axis=1)]\n",
    "\n",
    "    # check upper case\n",
    "    count = df_results.shape[0]\n",
    "    indexes = df_results.index\n",
    "\n",
    "    # count percent\n",
    "    df_percent = count / df_local.shape[0]\n",
    "\n",
    "    # print table\n",
    "    MASTER_TABLE += f\"{analyse_name}\\n\"\n",
    "    \n",
    "    # if there are no values exit\n",
    "    if df_percent == 0:\n",
    "        MASTER_TABLE += \"% of rows where the Value column does items not contain a number\\n\"\n",
    "        MASTER_TABLE += \"PASS\\n\"\n",
    "        MASTER_TABLE += \"-\\n\"\n",
    "        MASTER_TABLE += \"-\\n\"\n",
    "        MASTER_TABLE += \"-\\n\"\n",
    "\n",
    "        add_to_cmd_table(analyse_name, \"PASS\")\n",
    "        return\n",
    "    \n",
    "    add_to_cmd_table(analyse_name, \"FIX\", True)\n",
    "\n",
    "    table = [[\"% of rows where the Value column does items not contain a number\"],[df_percent]]\n",
    "    print_table(table, True)\n",
    "\n",
    "\n",
    "    # save document\n",
    "    df_full_table = dataFrame.loc[indexes]\n",
    "    df_full_table.to_csv(f'{FOLDER_PATH}/FORMATTING/Report-{analyse_name}-Output_1-{filename}-{CURRENT_DATE}.csv',encoding='utf-8')\n",
    "            \n",
    "format_2(df, [\"Value\"], \"Formatting_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ns/p_ft2qb57yjb3kyq4n0lr36r0000gn/T/ipykernel_4067/188355667.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mregex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"([^A-Za-z0-9 ])\\1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mdescription_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"% of rows with a doubled character that is not a letter or number in any column & count\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mformat_all_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Formatting_3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mregex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"([ ])\\1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/ns/p_ft2qb57yjb3kyq4n0lr36r0000gn/T/ipykernel_4067/188355667.py\u001b[0m in \u001b[0;36mformat_all_columns\u001b[0;34m(dataFrame, df_columns, regex, analyse_name, description_text)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# create a file name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mcn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[^A-Z]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIGNORECASE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mcn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' +'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "def format_all_columns(dataFrame, df_columns, regex, analyse_name, description_text):\n",
    "    global MASTER_TABLE\n",
    "\n",
    "    # drop source as it always has double characters\n",
    "    df_local = dataFrame[df_columns].drop(\"Source\", axis=1)\n",
    "\n",
    "    indexes = []\n",
    "    output_count = []\n",
    "\n",
    "    # go trough columns and check whether in any of those there is a questionmark\n",
    "    for index, item in enumerate(df_local.columns):\n",
    "        # stringify the column\n",
    "        # temp = dataFrame[item].astype(str).str.replace('.', '', regex=True)\n",
    "        temp = dataFrame[item].astype(str)\n",
    "        search_range = temp[temp.str.contains(regex)]\n",
    "        \n",
    "        # set indexes to search range ndexes\n",
    "        indexes = search_range.index.tolist()\n",
    "\n",
    "        # append to output count\n",
    "        output_count.append(str(len(indexes)))\n",
    "\n",
    "        # save file if found\n",
    "        if len(indexes) > 0:\n",
    "            # create a file name\n",
    "            cn = re.sub(\"[^A-Z]\", \" \", item,0,re.IGNORECASE)\n",
    "            cn = re.sub(' +', '_', cn)\n",
    "\n",
    "            # save document\n",
    "            df_full_table = dataFrame.loc[indexes]\n",
    "            df_full_table.to_csv(f'{FOLDER_PATH}/FORMATTING/Report-{analyse_name}-{cn}-Output_1-{filename}-{CURRENT_DATE}.csv',encoding='utf-8')\n",
    "\n",
    "    # calculate percentages\n",
    "    output_percentages = [int(i) / df_local.shape[0] for i in output_count]\n",
    "\n",
    "    # print table\n",
    "    MASTER_TABLE += f\"{analyse_name}\\n\"\n",
    "    MASTER_TABLE += f\"{description_text}\\n\"\n",
    "\n",
    "    # if there are no values exit\n",
    "    if  len([x for x in output_count if int(x) != 0]) == 0:\n",
    "        MASTER_TABLE += \"PASS\\n\"\n",
    "        MASTER_TABLE += \"-\\n\"\n",
    "        MASTER_TABLE += \"-\\n\"\n",
    "        MASTER_TABLE += \"-\\n\"\n",
    "        add_to_cmd_table(analyse_name, \"PASS\")\n",
    "        return\n",
    "    \n",
    "    if analyse_name == \"Formatting_5\":\n",
    "        add_to_cmd_table(analyse_name, \"FIX\", True)\n",
    "    else:\n",
    "        add_to_cmd_table(analyse_name, \"FIX\")\n",
    "    \n",
    "    table = [df_local.columns.tolist(), output_percentages, output_count]\n",
    "    print_table(table, False, [\"Column\", \"Percentage\", \"Count\"])\n",
    "\n",
    "regex = r\"([^A-Za-z0-9 ])\\1\"\n",
    "description_text = \"% of rows with a doubled character that is not a letter or number in any column & count\"\n",
    "format_all_columns(df, df.columns, regex, \"Formatting_3\", description_text)\n",
    "\n",
    "regex = r\"([ ])\\1\"\n",
    "description_text = \"% of rows with a doubled space in any column & count\"\n",
    "format_all_columns(df, df.columns, regex, \"Formatting_4\",description_text)\n",
    "\n",
    "regex = r\"([?])\"\n",
    "description_text = \"% of rows with a question mark in any column & count\"\n",
    "format_all_columns(df, df.columns, regex, \"Formatting_5\",description_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_table ():\n",
    "    global MASTER_TABLE\n",
    "    fname = f'{FOLDER_PATH}/Report-{filename}-{CURRENT_DATE}.csv'\n",
    "    with open(f'{fname}', 'w', newline='') as f_output:\n",
    "        f_output.write(MASTER_TABLE)\n",
    "save_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_files(directory):\n",
    "    # Go through directory and split files\n",
    "    # fname = f'{FOLDER_PATH}/Report-{filename}-{CURRENT_DATE}.csv'\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        # checking if it is a file\n",
    "        if os.path.isfile(f):\n",
    "            # check whether the file ending is csv\n",
    "            if f[-3:] == \"csv\":\n",
    "                # print(f)\n",
    "                # check size of the file\n",
    "                if os.path.getsize(f) > 50000000:\n",
    "                    print(f'splitting : {filename}')\n",
    "                    # make chunk out of big page\n",
    "                    chunkFolder = f\"{directory}/{filename[:-4]}/\"\n",
    "                    if not os.path.exists(chunkFolder):\n",
    "                        os.mkdir(chunkFolder)\n",
    "                    for i,chunk in enumerate(pd.read_csv(f, chunksize=250000)):\n",
    "                        chunk.to_csv(chunkFolder+f'{filename[:-4]}'+'-{}.csv'.format(i), index=False)\n",
    "                        print(i)\n",
    "                    # remove file from the system\n",
    "                    os.remove(f)\n",
    "        if os.path.isdir(f):\n",
    "            split_files(f)\n",
    "split_files(FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only make an archive if zip is True\n",
    "if zip:\n",
    "    shutil.make_archive(folder_to_download, 'zip', folder_to_download)\n",
    "    filelink = folder_to_download+'.zip'\n",
    "    FileLink(filelink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------+\n",
      "| Name                    | Value |\n",
      "+-------------------------+-------+\n",
      "| Duplication             | PASS  |\n",
      "| Non_discerning_info     | PASS  |\n",
      "| Missing_value_mandatory | PASS  |\n",
      "| Repetition_1            | FIX   |\n",
      "| Repetition_2            | FIX   |\n",
      "| Repetition_4            | FIX   |\n",
      "| Formatting-1            | PASS  |\n",
      "| Formatting_2            | PASS  |\n",
      "| Formatting_3            | PASS  |\n",
      "| Formatting_4            | PASS  |\n",
      "| Formatting_5            | PASS  |\n",
      "+-------------------------+-------+\n"
     ]
    }
   ],
   "source": [
    "# Column headers\n",
    "headers = [\"Name\", \"Value\"]\n",
    "colalign = (\"left\", \"left\")\n",
    "print(tabulate(CMD_TABLE, headers, tablefmt=\"pretty\", colalign=colalign))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
